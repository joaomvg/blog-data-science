---
layout: post
title: "Kernel Methods"
date: 2021-09-29
category: Machine Learning
image: 
excerpt: We explore the use of Kernels in classification, regression and probability distributions
katex: True
---

### Definition

A kernel $K(x,x')$ is a function that obeys the property

$$K(x,x')=\langle\Phi(x)\cdot\Phi(x')\rangle$$

where $\langle\cdot\rangle$ denotes inner product in some vector space $\mathbb{V}$. Here $\Phi(x)$ is a mapping from $x\in \mathbb{R}^d$ to the vector space $\mathbb{V}$. So for example, any polynomial of $\langle x\cdot x'\rangle$ is a kernel. This is because of the property

$$\begin{equation*}\begin{split}
&\langle x\cdot x'\rangle^p = \left(\sum_{i=1}^d x_i x'_i\right)^p =\sum_k C(k) x_i^k {x'}_i^k\\
&=(C(1)^{1/2}x_1,C(2)^{1/2}x_1^2,\ldots, C(1)^{1/2}x_2,\ldots)^{T}(C(1)^{1/2}{x'}_1,C(2)^{1/2}{x'}_1^2,\ldots,C(1)^{1/2} {x'}_2,\ldots)
\end{split}\end{equation*}$$

Using this property we can also show that the gaussian function is a kernel:

$$\exp{\left(-\gamma |x-x'|^2\right)}=\exp{\left(-\gamma x^2-\gamma {x'}^2-2\gamma \langle x\cdot x'\rangle\right)}=\exp{\left(-\gamma x^2-\gamma {x'}^2\right)} \sum_{n=1}^{\infty}\frac{(-2\langle x\cdot x'\rangle)^n}{n!}$$

### Regression

In the KNN algorithm we take the K nearest neighbors of point $x$ and average. That is:

$$\hat{y}|_x=\frac{1}{K}\sum_{i\in \text{K-neighbors(x)}} y_i|_x$$

To put it differently, we consider probabilities $p(y_i|x)=\frac{1}{K}$ and attach them to the neighbors of point $x$. 
The above average is simply

$$\hat{y}|_x=E(y|x)$$

But rather than giving equal weights to the neighbors, we can give weights that decay with distance. This allows us to include contributions from very far without introducing additional bias. For example, using the gaussian function:

$$p(y_i,x)=\exp{\left(-\frac{|x-x_i|^2}{d^2}\right)}$$

and 

$$p(y_i|x)=\frac{p(y_i,x)}{\sum_i p(y_i,x)}$$

Here $d$ is the Kernel width. Then the average becomes

$$E(y|x)=\frac{\sum_i y_ip(y_i,x)}{\sum_i p(y_i,x)}$$

As an example, we generate artificial data:
<div style="text-align: center"><img src="/images/kernel_reg_d.png"  width="100%"></div>

For small $d$ we see a lot of variance in the regression line and for larger values of $d$ the function changes very slowly. In fact, when $d$ is small the contributions from the nearest neighbors contribute more strongly and as such variance increases. But for large $d$ all the data-points start contributing equally which increases bias.

### Distributions

